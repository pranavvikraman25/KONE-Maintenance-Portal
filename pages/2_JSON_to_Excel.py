# pages/2_JSON_to_Excel.py
import json
import io
from docx import Document
from pathlib import Path
import pandas as pd
import streamlit as st
import os
from io import BytesIO
# -----------------------------------------------------------


UPLOAD_DIR = "backend/uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

def save_uploaded_file(uploaded_file):
    """Save uploaded file both in memory and on disk."""
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    # store in session
    st.session_state['uploaded_file_path'] = file_path
    st.session_state['uploaded_file_name'] = uploaded_file.name
    return file_path

def get_uploaded_file():
    """Return file path if exists in session."""
    return st.session_state.get('uploaded_file_path', None)


# -----------------------------------------------------------
st.set_page_config(page_title="Live Elevator KPI Report", layout="wide")
st.title("üìä Live Elevator KPI Data ‚Üí Named Excel Report")
st.write("""
Upload your live elevator **JSON** or **Word (.docx)** file.  
The app automatically merges it with your fixed KPI names and produces a final Excel report.
""")

# -----------------------------------------------------------
# FILE LOCATIONS
# -----------------------------------------------------------
# Determine path dynamically (always works, even on Streamlit Cloud)
CURRENT_DIR = Path(__file__).parent
ASSETS_DIR = Path(__file__).parents[1] / "assets"
LOOKUP_FILE = ASSETS_DIR / "kpid_names.csv"

# Debug info (for troubleshooting ‚Äî you can comment this later)
st.caption(f"üîç Looking for lookup file at: `{LOOKUP_FILE.resolve()}`")

# -----------------------------------------------------------
# HELPER FUNCTIONS
# -----------------------------------------------------------

def load_fixed_lookup():
    """Load the KPI ID ‚Üí Name mapping file from /assets/kpid_names.csv."""
    if not LOOKUP_FILE.exists():
        st.error(f"‚ùå Lookup file not found at: {LOOKUP_FILE}")
        st.stop()
    try:
        df = pd.read_csv(LOOKUP_FILE, header=None)
        df.columns = ["ID", "Name"]
        df["ID"] = df["ID"].astype(str).str.strip()
        st.success(f"‚úÖ Loaded lookup file: {LOOKUP_FILE.name} with {len(df)} entries.")
        return df
    except Exception as e:
        st.error(f"‚ùå Failed to read lookup file: {e}")
        st.stop()

def read_json_from_word(uploaded_file):
    """Extract embedded JSON text from a Word (.docx) file."""
    try:
        doc = Document(uploaded_file)
        full_text = "\n".join(p.text for p in doc.paragraphs)
        json_start = full_text.find("{")
        json_end = full_text.rfind("}")
        if json_start != -1 and json_end != -1:
            json_str = full_text[json_start:json_end + 1]
            return json.loads(json_str)
        st.error("‚ö†Ô∏è No JSON content found inside the Word file.")
        return None
    except Exception as e:
        st.error(f"Failed to extract JSON from Word: {e}")
        return None

def find_messages(obj):
    """Locate list of dict messages inside JSON."""
    if isinstance(obj, list) and obj and isinstance(obj[0], dict):
        return obj
    if isinstance(obj, dict):
        for key in ["messages", "data", "records", "items"]:
            if key in obj and isinstance(obj[key], list):
                return obj[key]
    return None

def safe_load_json(uploaded_file):
    """Safe JSON loader for uploaded files."""
    try:
        return json.load(uploaded_file)
    except Exception as e:
        st.error(f"Failed to parse JSON: {e}")
        return None

# -----------------------------------------------------------
# FILE UPLOAD SECTION
# -----------------------------------------------------------

uploaded_file = st.file_uploader("üìÇ Upload live JSON or Word (.docx) file", type=["json", "docx"])
if not uploaded_file:
    st.info("Please upload a JSON or DOCX file to begin.")
    st.stop()

lookup_df = load_fixed_lookup()

with st.spinner("üîÑ Reading uploaded file..."):
    # Detect file type
    if uploaded_file.name.endswith(".docx"):
        data = read_json_from_word(uploaded_file)
    else:
        data = safe_load_json(uploaded_file)

    if data is None:
        st.stop()

    messages = find_messages(data)
    if messages is None:
        st.error("‚ö†Ô∏è Could not find 'messages' or list of data in JSON.")
        st.stop()

    df_raw = pd.DataFrame(messages)
    st.write("üìã Detected columns:", list(df_raw.columns))

    # Expected columns
    expected_cols = ["kpiId", "data", "floor", "timestamp"]
    available_cols = [c for c in expected_cols if c in df_raw.columns]
    missing = [c for c in expected_cols if c not in df_raw.columns]
    if missing:
        st.warning(f"‚ö†Ô∏è Missing keys: {missing}. Proceeding with available columns.")

    # Select and clean
    df_live = df_raw[available_cols].copy()
    rename_map = {
        "kpiId": "ID",
        "data": "KPI Value",
        "floor": "Floor",
        "timestamp": "Timestamp"
    }
    df_live = df_live.rename(columns=rename_map)
    df_live["ID"] = df_live["ID"].astype(str).str.strip()

    # Merge with lookup table
    merged = pd.merge(df_live, lookup_df, on="ID", how="left")

    # Arrange columns
    final_cols = ["Name", "ID", "KPI Value", "Floor", "Timestamp"]
    merged = merged[[c for c in final_cols if c in merged.columns]]

    # Sort by timestamp if numeric
    merged["Timestamp"] = pd.to_numeric(merged["Timestamp"], errors="coerce")
    merged = merged.sort_values(by="Timestamp", ascending=True)

    # Preview
    st.subheader("üìä Merged Data Preview")
    st.dataframe(merged.head(100), use_container_width=True)
    st.success(f"‚úÖ Processed {len(merged)} records successfully!")

    # Export Excel
    output_name = "Elevator_KPI_Final_Report.xlsx"
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
        merged.to_excel(writer, index=False, sheet_name="Final_Report")
    buffer.seek(0)

    st.download_button(
        label="‚¨á Download Final Excel Report",
        data=buffer.getvalue(),
        file_name=output_name,
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # KPI Summary
    if "Name" in merged.columns:
        st.markdown("### üìà Summary ‚Äî Records by KPI")
        summary = merged["Name"].value_counts().reset_index()
        summary.columns = ["KPI Name", "Count"]
        st.table(summary)




